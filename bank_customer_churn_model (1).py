# -*- coding: utf-8 -*-
"""Bank_customer_churn_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c3FC1ztZjgeYKrssdRqmB_bXze6AYhdw

**BANK CUSTOMER CHURN MODEL**

**Objectives**



1.   Data Encoding
2.   Feature Scaling
3.   Handling Imbalance Data
4.   Support Vector Machine Classifier
5.   Grid Search For Hyperparameter Tunning

**Import Library**
"""

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

"""**Import Data**"""

df = pd.read_csv('https://github.com/YBIFoundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv')

df.head()

"""**Describe Data**"""

df.info()

df.duplicated('CustomerId').sum()

df = df.set_index('CustomerId')

df.info()

"""**Data Visualization**"""

sns.countplot(x='Geography', data=df)
plt.title('Distribution of Customers by Geography')
plt.show()

sns.countplot(x='Gender', data=df)
plt.title('Distribution of Customers by Gender')
plt.show()

sns.countplot(x='Has Credit Card', data=df)
plt.title('Distribution of Credit Card Holders')
plt.show()

sns.countplot(x='Is Active Member', data=df)
plt.title('Distribution of Active Members')
plt.show()

sns.countplot(x='Churn', data=df)
plt.title('Customer Churn Distribution')
plt.show()

sns.countplot(x='Zero Balance', data=df)
plt.title('Distribution of Zero Balance Accounts')
plt.show()

corr_matrix = df.select_dtypes(include=[np.number]).corr()  # Select only numerical columns
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

sns.boxplot(x='Geography', y='Balance', data=df)
plt.title('Balance by Geography')
plt.show()

"""**Encoding**"""

df['Geography'].value_counts()

df.replace({'Geography':{'France':2,'Germany':1,'Spain':0}},inplace=True)

df['Gender'].value_counts()

df.replace({'Gender':{'Male':0,'Female':1}},inplace=True)

df['Num Of Products'].value_counts()

df.replace({'Num Of Products':{1:0,2:1,3:1,4:1}},inplace=True)

df['Has Credit Card'].value_counts()

df['Is Active Member'].value_counts()

df.loc[(df['Balance']==0),'Churn'].value_counts()

df['Zero Balance']= np.where(df['Balance']>0,1,0)

df['Zero Balance'].hist()

df.groupby(['Churn','Geography']).count()

"""**Define Target Variable (y) and Feature Variable X**"""

df.columns

X = df.drop(['Surname','Churn'],axis=1)

y = df['Churn']

X.shape

y.shape

from imblearn.under_sampling import RandomUnderSampler

sns.countplot(x='Churn',data=df);

"""**Handling Imbalance Data**


1.   UnderSampling
2.   Over Sampling

**Random UnderSampling**
"""

rus = RandomUnderSampler(random_state=2529)

X_rus,y_rus= rus.fit_resample(X,y)

X_rus.shape,y_rus.shape,X.shape,y.shape

y.value_counts()

y_rus.value_counts()

y_rus.plot(kind = 'hist')

"""**Random Over Sampling**"""

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state= 2529)

X_ros,y_ros = ros.fit_resample(X,y)

X_ros.shape,y_ros.shape,X.shape,y.shape

y.value_counts()

y_ros.plot(kind ='hist')

"""**Train Test Split**"""

from sklearn.model_selection import train_test_split

"""**Split Original Data**"""

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=25)

"""**Split Random Under Sample Data**"""

X_train_ros,X_test_ros,y_train_ros,y_test_ros = train_test_split(X_ros,y_ros,test_size=0.3,random_state=25)

"""**Split Random Over Sample Data**"""

X_train_rus,X_test_rus,y_train_rus,y_test_rus = train_test_split(X_rus,y_rus,test_size=0.3,random_state=25)

"""**Standardize Features**"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

"""**Standardize Original Data**"""

X_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(X_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

X_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(X_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""**Standardize Random Over Sample Data**"""

X_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(X_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

X_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(X_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""**Standardize Random Under Sample Data**"""

X_train_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(X_train_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

X_test_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']] = sc.fit_transform(X_test_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""**Support Vector Machine Classifier**"""

from sklearn.svm import SVC

svc = SVC()

svc.fit(X_train,y_train)

y_pred = svc.predict(X_test)

"""**Model Accuracy**"""

from sklearn.metrics import confusion_matrix,classification_report

confusion_matrix(y_test,y_pred)

print(classification_report(y_test,y_pred))

"""**Hyperparameter Tunning**"""

from sklearn.model_selection import GridSearchCV

param_grid = {'C':[0.1,1,10],
              'gamma':[1,0.1,0.01],
              'kernel':['rbf'],
              'class_weight':['balanced']}

grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)
grid.fit(X_train,y_train)

print(grid.best_estimator_)

grid_predictions = grid.predict(X_test)

confusion_matrix(y_test,grid_predictions)

print(classification_report(y_test,grid_predictions))

"""**Hyperparameter Tunning (Random Under Sample Data)**"""

svc_rus = SVC()

svc_rus.fit(X_train_rus,y_train_rus)

y_pred_rus = svc_rus.predict(X_test_rus)

confusion_matrix(y_test_rus,y_pred_rus)

print(classification_report(y_test_rus,y_pred_rus))

param_grid = {'C':[0.1,1,10],
              'gamma':[1,0.1,0.01],
              'kernel':['rbf'],
              'class_weight':['balanced']}

grid_rus = GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)
grid_rus.fit(X_train_rus,y_train_rus)

print(grid_rus.best_estimator_)

grid_predictions_rus = grid_rus.predict(X_test_rus)

confusion_matrix(y_test_rus,grid_predictions_rus)

print(classification_report(y_test_rus,grid_predictions_rus))

"""**Hyperparameter Tunning (Random Over Sampled Data)**"""

svc_ros = SVC()

svc_ros.fit(X_train_ros,y_train_ros)

y_pred_ros = svc_ros.predict(X_test_ros)

confusion_matrix(y_test_ros,y_pred_ros)

print(classification_report(y_test_ros,y_pred_ros))

param_grid = {'C':[0.1,1,10],
              'gamma':[1,0.1,0.01],
              'kernel':['rbf'],
              'class_weight':['balanced']}

grid_ros = GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)
grid_ros.fit(X_train_ros,y_train_ros)

print(grid_ros.best_estimator_)

grid_predictions_ros = grid_ros.predict(X_test_ros)

confusion_matrix(y_test_ros,grid_predictions_ros)

print(classification_report(y_test_ros,grid_predictions_ros))

"""**Summary**"""

print(classification_report(y_test,y_pred))

print(classification_report(y_test,grid_predictions))

print(classification_report(y_test_rus,y_pred_rus))

print(classification_report(y_test_rus,grid_predictions_rus))

print(classification_report(y_test_ros,y_pred_ros))

print(classification_report(y_test_ros,grid_predictions_ros))

"""**Explanation**

**Data** **Encoding**:

This process converts categorical data, such as 'Male' or 'Female', into numerical values that machine learning models can understand. This is essential for model training and accuracy.

**Feature** **Scaling**:

To ensure that features contribute equally to the model, feature scaling brings all numerical data within a specific range. This prevents features with larger values from dominating the model's decision-making process.

**Handling** **Imbalance** **Data**:

When one class in a dataset significantly outweighs the other, it can lead to biased models. Techniques like oversampling or undersampling are used to balance the dataset, improving model performance and preventing overfitting.

**Support** **Vector** **Machine** (**SVM**):

SVM is a classification algorithm that creates a decision boundary to separate data points into different classes. It focuses on finding the optimal hyperplane that maximizes the margin between these classes, leading to better generalization.

**Grid** **Search** **for** **Hyperparameter** **Tuning**:

To optimize model performance, grid search systematically evaluates different combinations of hyperparameters to find the best configuration. This helps in achieving the highest accuracy and robustness of the model.
"""